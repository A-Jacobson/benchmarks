{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6219ee94-940d-4328-891a-2cb2827ec022",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import build_prompt_dataspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "72eef077-f291-40ac-999e-be3c36916be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ['meow', 'cat', 'floof'] * 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a10de6bb-c8d4-42c4-80c5-1a739400e18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataspec = build_prompt_dataspec(prompt, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da2617e2-cb37-45e0-b0cf-01d5070c7b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['meow', 'cat']\n",
      "['floof']\n"
     ]
    }
   ],
   "source": [
    "for batch in dataspec.dataloader:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3f264b0f-02a2-4425-a3fc-e67252735a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "114fbf06-b883-4674-af5d-8c424a3db0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_list(l, microbatch_size: int): # needed for gradient accumulation\n",
    "    if len(l) < microbatch_size:\n",
    "        microbatch_size = len(l)\n",
    "    num_microbatches = math.ceil(len(l) / microbatch_size)\n",
    "    # Note: this is to match the behavior of tensor.chunk, which is used in _split_tensor\n",
    "    chunked_microbatch_size = math.ceil(len(l) / num_microbatches)\n",
    "    return [l[start:start + chunked_microbatch_size] for start in range(0, len(l), chunked_microbatch_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fefa008c-f26e-40bc-850f-907164e46d6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['meow', 'cat'],\n",
       " ['floof', 'meow'],\n",
       " ['cat', 'floof'],\n",
       " ['meow', 'cat'],\n",
       " ['floof', 'meow'],\n",
       " ['cat', 'floof'],\n",
       " ['meow', 'cat'],\n",
       " ['floof']]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_list(prompt, microbatch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6e448b80-ed50-406c-b5a5-056bec8bd69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "933f772a-b0dd-4130-98c9-f11be517df0b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many dimensions 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/79/3kl5b0ss1035q1pd2bj6rsjh0000gn/T/ipykernel_19236/1711609106.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: too many dimensions 'str'"
     ]
    }
   ],
   "source": [
    "torch.Tensor(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "151278c2-9d95-4a90-b211-888bdce75370",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many dimensions 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/79/3kl5b0ss1035q1pd2bj6rsjh0000gn/T/ipykernel_19236/1711609106.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: too many dimensions 'str'"
     ]
    }
   ],
   "source": [
    "torch.Tensor(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc04dfa1-4568-4928-b096-bbce1e29ecd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import (AutoencoderKL, DDPMScheduler, LMSDiscreteScheduler,\n",
    "                       UNet2DConditionModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "daf56300",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet2DConditionModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a453fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "model_name_or_path = \"CompVis/stable-diffusion-v1-4\"\n",
    "model = UNet2DConditionModel.from_pretrained(model_name_or_path, subfolder='unet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "064026fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'AutoConfig' from 'diffusers' (/Users/austin/anaconda3/lib/python3.9/site-packages/diffusers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdiffusers\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoConfig\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'AutoConfig' from 'diffusers' (/Users/austin/anaconda3/lib/python3.9/site-packages/diffusers/__init__.py)"
     ]
    }
   ],
   "source": [
    "from diffusers import AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13ae7fb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "857a8ccc441442bf8a1f8cd6d2f4ff20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/743 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Unrecognized model in CompVis/stable-diffusion-v1-4. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: albert, altclip, audio-spectrogram-transformer, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, blenderbot, blenderbot-small, blip, bloom, camembert, canine, chinese_clip, clip, clipseg, codegen, conditional_detr, convbert, convnext, ctrl, cvt, data2vec-audio, data2vec-text, data2vec-vision, deberta, deberta-v2, decision_transformer, deformable_detr, deit, detr, dinat, distilbert, donut-swin, dpr, dpt, efficientformer, electra, encoder-decoder, ernie, esm, flaubert, flava, fnet, fsmt, funnel, git, glpn, gpt-sw3, gpt2, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, graphormer, groupvit, hubert, ibert, imagegpt, jukebox, layoutlm, layoutlmv2, layoutlmv3, led, levit, lilt, longformer, longt5, luke, lxmert, m2m_100, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, megatron-bert, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mpnet, mt5, mvp, nat, nezha, nystromformer, oneformer, openai-gpt, opt, owlvit, pegasus, pegasus_x, perceiver, plbart, poolformer, prophetnet, qdqbert, rag, realm, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, segformer, sew, sew-d, speech-encoder-decoder, speech_to_text, speech_to_text_2, splinter, squeezebert, swin, swin2sr, swinv2, switch_transformers, t5, table-transformer, tapas, time_series_transformer, timesformer, trajectory_transformer, transfo-xl, trocr, unispeech, unispeech-sat, upernet, van, videomae, vilt, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, wav2vec2, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, yolos, yoso",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m AutoConfig\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39;49m\u001b[39mCompVis/stable-diffusion-v1-4\u001b[39;49m\u001b[39m\"\u001b[39;49m, subfolder\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39munet\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/models/auto/configuration_auto.py:882\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[39mif\u001b[39;00m pattern \u001b[39min\u001b[39;00m \u001b[39mstr\u001b[39m(pretrained_model_name_or_path):\n\u001b[1;32m    880\u001b[0m             \u001b[39mreturn\u001b[39;00m CONFIG_MAPPING[pattern]\u001b[39m.\u001b[39mfrom_dict(config_dict, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39munused_kwargs)\n\u001b[0;32m--> 882\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    883\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized model in \u001b[39m\u001b[39m{\u001b[39;00mpretrained_model_name_or_path\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    884\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mShould have a `model_type` key in its \u001b[39m\u001b[39m{\u001b[39;00mCONFIG_NAME\u001b[39m}\u001b[39;00m\u001b[39m, or contain one of the following strings \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    885\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39min its name: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(CONFIG_MAPPING\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    886\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Unrecognized model in CompVis/stable-diffusion-v1-4. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: albert, altclip, audio-spectrogram-transformer, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, blenderbot, blenderbot-small, blip, bloom, camembert, canine, chinese_clip, clip, clipseg, codegen, conditional_detr, convbert, convnext, ctrl, cvt, data2vec-audio, data2vec-text, data2vec-vision, deberta, deberta-v2, decision_transformer, deformable_detr, deit, detr, dinat, distilbert, donut-swin, dpr, dpt, efficientformer, electra, encoder-decoder, ernie, esm, flaubert, flava, fnet, fsmt, funnel, git, glpn, gpt-sw3, gpt2, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, graphormer, groupvit, hubert, ibert, imagegpt, jukebox, layoutlm, layoutlmv2, layoutlmv3, led, levit, lilt, longformer, longt5, luke, lxmert, m2m_100, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, megatron-bert, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mpnet, mt5, mvp, nat, nezha, nystromformer, oneformer, openai-gpt, opt, owlvit, pegasus, pegasus_x, perceiver, plbart, poolformer, prophetnet, qdqbert, rag, realm, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, segformer, sew, sew-d, speech-encoder-decoder, speech_to_text, speech_to_text_2, splinter, squeezebert, swin, swin2sr, swinv2, switch_transformers, t5, table-transformer, tapas, time_series_transformer, timesformer, trajectory_transformer, transfo-xl, trocr, unispeech, unispeech-sat, upernet, van, videomae, vilt, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, wav2vec2, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, yolos, yoso"
     ]
    }
   ],
   "source": [
    "AutoConfig.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder='unet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fecf9314",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"sample_size\": 64, \"in_channels\": 4, \"out_channels\": 4, \"center_input_sample\": false, \"flip_sin_to_cos\": true, \"freq_shift\": 0, \"down_block_types\": [\"CrossAttnDownBlock2D\", \"CrossAttnDownBlock2D\", \"CrossAttnDownBlock2D\", \"DownBlock2D\"], \"mid_block_type\": \"UNetMidBlock2DCrossAttn\", \"up_block_types\": [\"UpBlock2D\", \"CrossAttnUpBlock2D\", \"CrossAttnUpBlock2D\", \"CrossAttnUpBlock2D\"], \"only_cross_attention\": false, \"block_out_channels\": [320, 640, 1280, 1280], \"layers_per_block\": 2, \"downsample_padding\": 1, \"mid_block_scale_factor\": 1, \"act_fn\": \"silu\", \"norm_num_groups\": 32, \"norm_eps\": 1e-05, \"cross_attention_dim\": 768, \"attention_head_dim\": 8, \"dual_cross_attention\": false, \"use_linear_projection\": false, \"class_embed_type\": null, \"num_class_embeds\": null, \"upcast_attention\": false, \"resnet_time_scale_shift\": \"default\", \"_class_name\": \"UNet2DConditionModel\", \"_diffusers_version\": \"0.2.2\", \"_name_or_path\": \"CompVis/stable-diffusion-v1-4\"}'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "json.dumps(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0147534",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PretrainedConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78018329",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = PretrainedConfig.get_config_dict(\"CompVis/stable-diffusion-v1-4\", subfolder='unet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4b93a612",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import  UNet2DConditionModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "519910af",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet2DConditionModel.from_config(config[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9992eb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import build_prompt_dataspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6d2e7b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = build_prompt_dataspec(['doge', 'cate', 'snail', 'duck'], batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f79d681f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['doge', 'cate']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(ds.dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ea077691",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.utils import make_grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "42210022",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = torch.randn(size=(4, 3, 512, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3afa58d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = images.chunk(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7056683f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 512, 512])\n",
      "torch.Size([2, 3, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "for chunk in chunks:\n",
    "    print(chunk.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c035102a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "5580219ee5e1ad82cd38c31a44ed6f0cd10f0c31df7e40701d173b0918d6667d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
