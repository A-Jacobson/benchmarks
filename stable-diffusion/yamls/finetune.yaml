run_name: stable-diffusion-finetune # Name of the  training run used for checkpointing and other logging
seed: 42               # Random seed
max_duration: '10000ba'    # Duration to train specified as a Time string
grad_accum: 'auto'       # Amount of gradient accumulation, 'auto' (GPU only) means Composer will set the optimal value, set to 1 for cpu
use_ema: false
eval_interval: '1000ba'

dataset:
  name: "lambdalabs/pokemon-blip-captions" # other datasets can be found at https://huggingface.co/datasets?task_categories=task_categories:text-to-image
  resolution: 512        # training image size
  image_column: 'image'
  caption_column: 'text'
  mean:
    - 0.5                 # default chanel normalization for pokemon dataset, may need to be changed for other datasets
  std:
    - 0.5                 # default chanel normalization for pokemon dataset, may need to be changed for other datasets
  train_batch_size: 32    # Training total batch size (4 per 40gb A100)
  eval_batch_size: 32
  prompts:
    - "Doge"
    - "A cute bunny rabbit"
    - "Yoda"
    - "An epic landscape photo of a mountain"
    - "Girl with a pearl earring"
    - "Cute Obama creature"
    - "Donald Trump"
    - "Boris Johnson"
    - "Totoro"
    - "Hello Kitty"

model:
  name: "CompVis/stable-diffusion-v1-4" # commonly "CompVis/stable-diffusion-v1-4" or "stabilityai/stable-diffusion-2-1" use 2-1 for sizes > 512
  train_text_encoder: false
  num_images_per_prompt: 4  # number of images to generate per prompt
  image_key: "image_tensor"
  caption_key: "input_ids"


optimizer:
  lr: 1.0e-04
  weight_decay: 1.0e-2


loggers:
  progress_bar: {}
  # wandb:     # Uncomment and fill below arguments to use WandB logger
  #   entity:  # Name of WandB entity, usually username or organization name
  #   project: 'stable-diffusion-finetune' # Name of WandB project
  #   group:   # Name of WandB group

# Save checkpoint parameters
save_folder:                    # e.g. './{run_name}/ckpt' (local) or 's3://mybucket/mydir/{run_name}/ckpt' (remote)
save_interval: '300ba'             # Interval to checkpoint based on time string
save_num_checkpoints_to_keep: 5 # Cleans up checkpoints saved locally only!

# Load checkpoint parameters
load_path:      # e.g. './ckpt/latest-rank{rank}.pt' (local) or 's3://mybucket/mydir/ckpt/latest-rank{rank}.pt' (remote)
